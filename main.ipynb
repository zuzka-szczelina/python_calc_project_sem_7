{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e9f43980-e1a5-4f0f-a62f-148ddedb8b9d",
      "metadata": {
        "id": "e9f43980-e1a5-4f0f-a62f-148ddedb8b9d"
      },
      "source": [
        "# Final Project\n",
        "\n",
        "I'm using an exampple from the book \\'Machine Learning with PyTorch and Scikit-Learn\\' by Sebastian Raschka (Chapter 15, Project two - character-level language modeling in PyTorch) to create a RNN model and train it to generate short statement-like texts on the basis of Albert Einstein's book \\'Relativity : the Special and General Theory\\'. My goal is to create a functionality resambling asking a geeky friend who read the book to express their views on a chosen topic.\n",
        "\n",
        "To accelerate the model training I used Google Colab environment where I specified Runtime type as *Python 3* and Hardware accelerator as *T4 GPU*\n",
        "\n",
        "This notebook is adapted to be run in Google Colab environment.\\\n",
        "Model training section is included to show how I trained the model. You can decide whether you want to perform the training yourels of load the results of the traing I did by choosing between **perform_model_training** and **load_pretrained_model** modes below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "d7MbHejEnSmc",
      "metadata": {
        "id": "d7MbHejEnSmc"
      },
      "outputs": [],
      "source": [
        "# training_usecase = 'perform_model_training'\n",
        "training_usecase = 'load_pretrained_model'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bdp2btynuSH",
      "metadata": {
        "id": "7bdp2btynuSH"
      },
      "source": [
        "\n",
        "## 1. Required imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "1b12e135-e52f-4529-9ea2-1faba71a5c81",
      "metadata": {
        "id": "1b12e135-e52f-4529-9ea2-1faba71a5c81"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.distributions.categorical import Categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2U3P_MzyS4Nx",
      "metadata": {
        "id": "2U3P_MzyS4Nx"
      },
      "source": [
        "Here I'm checking Google Colab module instalation and whether hardware accelerator was set correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "Ea4Lelz8Saav",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea4Lelz8Saav",
        "outputId": "739d1201-b7a1-4e17-b0a8-fd41da54f666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version:  2.5.1+cu121\n",
            "GPU Available: False\n",
            "device:  cpu\n"
          ]
        }
      ],
      "source": [
        "print('torch version: ',torch.__version__)\n",
        "\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "print('device: ', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b67ae47-b1b7-4ddf-ac13-97d37bb74495",
      "metadata": {
        "id": "2b67ae47-b1b7-4ddf-ac13-97d37bb74495"
      },
      "source": [
        "## 2. Data preparation\n",
        "I obtained the book text in the .txt format form https://www.gutenberg.org/files/5001/old/2004-5001.txt and saved it as \\'Einstein_relativity_book.txt\\'.\n",
        "\n",
        "Below there are 2 code versions:\\\n",
        "version_1: avoiding clonning a GitHub repo (files are loaded directly from my GitHub repo)\\\n",
        "version_2: including GitHub repo cloning (if you use Google Colab repo is cloned to Google Colab not your local system)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "CApjVgLStdUd",
      "metadata": {
        "id": "CApjVgLStdUd"
      },
      "outputs": [],
      "source": [
        "data_loading_mode = 'clone_github_repo'\n",
        "# data_loading_mode = 'load_from_github'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eyTJkPaft4zs",
      "metadata": {
        "id": "eyTJkPaft4zs"
      },
      "source": [
        "###version_1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "5ccea00e-dbf9-43f4-b026-160272b123ea",
      "metadata": {
        "id": "5ccea00e-dbf9-43f4-b026-160272b123ea"
      },
      "outputs": [],
      "source": [
        "if data_loading_mode == 'load_from_github':\n",
        "    book_url = 'https://github.com/zuzka-szczelina/python_calc_project_sem_7/raw/refs/heads/master/data/Einstein_relativity_book.txt'\n",
        "    response = requests.get(book_url)\n",
        "    text = response.text\n",
        "\n",
        "    start_idx = text.find('CONTENTS')\n",
        "    end_idx = text.find('END OF THE PROJECT GUTENBERG')\n",
        "    text = text[start_idx: end_idx]\n",
        "    char_set = set(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V_EN0pZ4nT4i",
      "metadata": {
        "id": "V_EN0pZ4nT4i"
      },
      "source": [
        "###version_2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "wi3MZ1UfHOw8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi3MZ1UfHOw8",
        "outputId": "72b6acfd-58ba-4f93-be36-2e2ad427dafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'python_calc_project_sem_7'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 28 (delta 12), reused 10 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (28/28), 5.90 MiB | 10.55 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n",
            "/content/python_calc_project_sem_7/python_calc_project_sem_7\n"
          ]
        }
      ],
      "source": [
        "if data_loading_mode == 'clone_github_repo':\n",
        "    !git clone https://github.com/zuzka-szczelina/python_calc_project_sem_7.git\n",
        "    %cd python_calc_project_sem_7\n",
        "\n",
        "    with open('./data/Einstein_relativity_book.txt', 'r', encoding='cp1252') as f:\n",
        "        text = f.read()\n",
        "        start_idx = text.find('CONTENTS')\n",
        "        end_idx = text.find('END OF THE PROJECT GUTENBERG')\n",
        "        text = text[start_idx: end_idx]\n",
        "        char_set = set(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "71ec8baf-7915-4cbb-b861-613a0dd9c754",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71ec8baf-7915-4cbb-b861-613a0dd9c754",
        "outputId": "4c9bcc12-b909-4fbc-e747-0782ecf0c898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Book text includes:\n",
            "total characters: 186305\n",
            "unique characters 87\n"
          ]
        }
      ],
      "source": [
        "print('Book text includes:')\n",
        "print(f'total characters: {len(text)}\\nunique characters {len(char_set)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "0f0f5bd7-2d63-464f-af1d-897196d9c331",
      "metadata": {
        "id": "0f0f5bd7-2d63-464f-af1d-897196d9c331"
      },
      "outputs": [],
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int_encoding = {ch: i for i, ch in enumerate(chars_sorted)}\n",
        "char_array = np.array(chars_sorted)\n",
        "text_encoded = np.array(\n",
        "    [char2int_encoding[ch] for ch in text],\n",
        "    dtype=np.int32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "8215464e-af96-431f-b149-fc0cdc6f4446",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8215464e-af96-431f-b149-fc0cdc6f4446",
        "outputId": "c2ac2d48-d276-4f99-bb12-63cec560e535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numerical code: [30 42 41 47 32 41 47 46  0  0 43 74 61 62 57 59 61]\n",
            " stands for ['C' 'O' 'N' 'T' 'E' 'N' 'T' 'S' '\\n' '\\n' 'P' 'r' 'e' 'f' 'a' 'c' 'e']\n"
          ]
        }
      ],
      "source": [
        "print(f'numerical code: {text_encoded[0:17]}\\n stands for {char_array[text_encoded[0:17]]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df5ab9e8-d555-4564-9b13-e1acb82f1d0f",
      "metadata": {
        "id": "df5ab9e8-d555-4564-9b13-e1acb82f1d0f"
      },
      "source": [
        "## 3. ML Dataset construction\n",
        "Divide text into chunks to feed into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "3a778319-d821-44c4-bd59-73ce26cb928b",
      "metadata": {
        "id": "3a778319-d821-44c4-bd59-73ce26cb928b"
      },
      "outputs": [],
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "text_chunks = [text_encoded[i: i + chunk_size] for i in range(len(text_encoded) - chunk_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "dc6376e8-a7cb-4613-815c-128c7192fdff",
      "metadata": {
        "id": "dc6376e8-a7cb-4613-815c-128c7192fdff"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "d72533db-82e8-4212-91c0-a190339b9cca",
      "metadata": {
        "id": "d72533db-82e8-4212-91c0-a190339b9cca"
      },
      "outputs": [],
      "source": [
        "seq_dataset = TextDataset(torch.tensor(np.array(text_chunks)))\n",
        "# seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "bd585cfd-dcfe-4950-8272-66f7139bf83e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd585cfd-dcfe-4950-8272-66f7139bf83e",
        "outputId": "211231c3-97b3-4ea1-ac31-5fecafa0aa19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model input (x):  'CONTENTS\\n\\nPreface\\n\\nPart I: The Special T'\n",
            "Model target (y): 'ONTENTS\\n\\nPreface\\n\\nPart I: The Special Th'\n",
            "Model input (x):  'ONTENTS\\n\\nPreface\\n\\nPart I: The Special Th'\n",
            "Model target (y): 'NTENTS\\n\\nPreface\\n\\nPart I: The Special The'\n",
            "Model input (x):  'NTENTS\\n\\nPreface\\n\\nPart I: The Special The'\n",
            "Model target (y): 'TENTS\\n\\nPreface\\n\\nPart I: The Special Theo'\n",
            "Model input (x):  'TENTS\\n\\nPreface\\n\\nPart I: The Special Theo'\n",
            "Model target (y): 'ENTS\\n\\nPreface\\n\\nPart I: The Special Theor'\n",
            "Model input (x):  'ENTS\\n\\nPreface\\n\\nPart I: The Special Theor'\n",
            "Model target (y): 'NTS\\n\\nPreface\\n\\nPart I: The Special Theory'\n",
            "Model input (x):  'NTS\\n\\nPreface\\n\\nPart I: The Special Theory'\n",
            "Model target (y): 'TS\\n\\nPreface\\n\\nPart I: The Special Theory '\n"
          ]
        }
      ],
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    print('Model input (x): ', repr(''.join(char_array[seq])))\n",
        "    print('Model target (y):', repr(''.join(char_array[target])))\n",
        "    if i==5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ff935b-8261-4517-8f78-2cbba6b5341e",
      "metadata": {
        "id": "45ff935b-8261-4517-8f78-2cbba6b5341e"
      },
      "source": [
        "create a dataloader - object used to pass data into the model in the form of \\'batches\\' (gropus of specified size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "66fee067-247f-4eae-bb65-711a3842bf68",
      "metadata": {
        "id": "66fee067-247f-4eae-bb65-711a3842bf68"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 64\n",
        "seq_dataloader = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "590935f5-6a92-4ebc-8022-58ac3d1ff9ae",
      "metadata": {
        "id": "590935f5-6a92-4ebc-8022-58ac3d1ff9ae"
      },
      "source": [
        "## 4. Defining a RNN model\n",
        "\n",
        "#### __init__ method:\n",
        "\n",
        "**self.embeding** is a table that sotres an embedding vector for each unique character (we generate one embedding vector for each character by specifying vocab_size=len(char_array) below). Each embeding vector has length of embed_dim\n",
        "\n",
        "**self.rnn** is a RNN we will use. We specify its properties using *torch.nn.LSTM* function. LSTM stands for \\'Long Short-Term Memory\\' end indicates that we will use a RNN with LSTM cells used as hidden layers. The input to a hidden lauer will be a specific character represented as an embedding vector. Therefore, we sepcify that we expect an input to be of size of the embeding vector length (embed_dim).\\\n",
        "Output of the model is:\\\n",
        "*output_features,\\\n",
        "(final hidden state (for each element in the sequence),\\\n",
        "final cell state (for each element in the sequence))*\n",
        "\n",
        "**self.fc** is where we define a type of transformation we will apply to the output of hidden layers (????)\n",
        "\n",
        "**self.rnn_hidden_size** is the number of features in the hidden state of RNN\n",
        "\n",
        "#### forward method:\n",
        "...\n",
        "\n",
        "#### init_hidden method:\n",
        "Is where we initialise the state of a hidden layer and LSTM cell. (which will be used in forward method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "3db9ef83-3367-4b4f-a16a-1aa8be171556",
      "metadata": {
        "id": "3db9ef83-3367-4b4f-a16a-1aa8be171556"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.LSTM(input_size=embed_dim,\n",
        "                           hidden_size=rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
        "                            out_features=vocab_size)\n",
        "\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dade0438-2cd6-4816-9912-c66958812ed0",
      "metadata": {
        "id": "dade0438-2cd6-4816-9912-c66958812ed0"
      },
      "source": [
        "## 5. Creating an istance of the defined model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "6b111a04-4fe8-41fe-8b6f-1a826c908016",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b111a04-4fe8-41fe-8b6f-1a826c908016",
        "outputId": "b55816c7-7df0-4d43-fea9-098c1679a96b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(87, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=87, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9298a0ef-0e29-4507-b71e-32befe1d6888",
      "metadata": {
        "id": "9298a0ef-0e29-4507-b71e-32befe1d6888"
      },
      "source": [
        "define loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "55a83abf-d50a-49d2-9a76-286c06b34c8d",
      "metadata": {
        "id": "55a83abf-d50a-49d2-9a76-286c06b34c8d"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374a6773-6145-4bc7-878b-81bf51972b49",
      "metadata": {
        "id": "374a6773-6145-4bc7-878b-81bf51972b49"
      },
      "source": [
        "## 6. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "d7942dfc-5a3f-4c32-a449-c683e59f249c",
      "metadata": {
        "id": "d7942dfc-5a3f-4c32-a449-c683e59f249c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "if training_usecase == 'perform_model_training':\n",
        "\n",
        "    num_epochs = 10000\n",
        "    # num_epochs = 30\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        hidden, cell = model.init_hidden(batch_size)\n",
        "        seq_batch, target_batch = next(iter(seq_dataloader))\n",
        "        seq_batch = seq_batch.to(device)\n",
        "        target_batch = target_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = 0\n",
        "        for c in range(seq_length):\n",
        "            pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "            loss += loss_fn(pred, target_batch[:, c])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss = loss.item()/seq_length\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'Epoch {epoch} loss: {loss:.4f}')\n",
        "    print('time passed: ', time.time() - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2bd47fb-7da7-4d61-acb7-9de96b70ee55",
      "metadata": {
        "id": "f2bd47fb-7da7-4d61-acb7-9de96b70ee55"
      },
      "source": [
        "In the above cell we ..\n",
        "for each epoch:\n",
        "1. hidden layer and cell states initialisation\n",
        "2. use seq_dataloader as iterator object and load one batch (set of 64 input sequences and corresponding target sequences)\n",
        "3. reset the gradients of all optimised tensors.\n",
        "4. initialise loss (between predicted seqences and target seqences of loaded batch) as 0\n",
        "5. we use for loop to:\\\n",
        "   I. predict next character for each character in the input_sequence. It is done simultaneously for all input sequences in the batch.\\\n",
        "   II.Compute temporary loss as sum of losses for all characters\n",
        "6. Compute loss gradients after iterating throuch all characters.\n",
        "7. Perform optimization step to update model parameters (function .step() can be called once the gradients are computed using .backward())\n",
        "8. Compute final loss for a batch (dividing by the number of characters each sequence had)\n",
        "9. Printing current loss updates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9RJvCrlMd-f0",
      "metadata": {
        "id": "9RJvCrlMd-f0"
      },
      "source": [
        "##7. Model saving and loading\n",
        "\n",
        "There are 2 ways to save model training results:\\\n",
        "**method 1.** : saving the whole model object (i.e. model object of a specified architecture and its learned parameters)\\\n",
        "**method 2.** : saving just parameters (to reuse the parameters one needs to create a model object of the same architecture as the trained one and load them into the model)\n",
        "\n",
        "model.eval() function is called to indicate that the model will now be used in evaluation mode (i.e. for inference). It's becaues some layers behave differently during training and inference and need their mode to be set in advace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "RTG9MrqNleXJ",
      "metadata": {
        "id": "RTG9MrqNleXJ"
      },
      "outputs": [],
      "source": [
        "saving_method = 'method_1'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oqyrXOF4lnKr",
      "metadata": {
        "id": "oqyrXOF4lnKr"
      },
      "source": [
        "### method 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "-hXJv7uAcfQt",
      "metadata": {
        "id": "-hXJv7uAcfQt"
      },
      "outputs": [],
      "source": [
        "if training_usecase == 'perform_model_training' and saving_method == 'method_1':\n",
        "    torch.save(model, 'data/self_trained_model.pt')\n",
        "\n",
        "    trained_model = torch.load('data/self_trained_model.pt', weights_only=False)\n",
        "    trained_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vYzdBvJFlL90",
      "metadata": {
        "id": "vYzdBvJFlL90"
      },
      "source": [
        "### method 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "rKH3oxOQdJGv",
      "metadata": {
        "id": "rKH3oxOQdJGv"
      },
      "outputs": [],
      "source": [
        "if training_usecase == 'perform_model_training' and saving_method == 'method_2':\n",
        "    torch.save(model.state_dict(), 'data/self_trained_model_state_dict.pt')\n",
        "\n",
        "    trained_model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "    trained_model.load_state_dict(torch.load('data/self_trained_model_state_dict.pt'))\n",
        "    trained_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "QLkZkXF9lGBQ",
      "metadata": {
        "id": "QLkZkXF9lGBQ"
      },
      "outputs": [],
      "source": [
        "if training_usecase == 'load_pretrained_model':\n",
        "    if  data_loading_mode == 'clone_github_repo':\n",
        "        trained_model = torch.load('data/pretrained_model.pt', map_location=device, weights_only=False)\n",
        "    if data_loading_mode == 'load_from_github':\n",
        "        trained_model = torch.load('https://github.com/zuzka-szczelina/python_calc_project_sem_7/raw/refs/heads/master/data/pretrained_model.pt', map_location=device)\n",
        "    trained_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WprMXBslykDt",
      "metadata": {
        "id": "WprMXBslykDt"
      },
      "source": [
        "## 8. Text generating function\n",
        "\n",
        "Below function is defined that utilises the model to generate text on the basis of the book.\n",
        "\n",
        "The rate to with a generated text may be meaningful can be altered by changing a **predictability_factor** - the bigger the more predictible (and likely more meaningful) the generated text will be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "f47ff58d-4652-43ec-956b-58ea45da5297",
      "metadata": {
        "id": "f47ff58d-4652-43ec-956b-58ea45da5297"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, starting_str, len_generated_text=500, predictability_factor=2.0):\n",
        "    encoded_input = torch.tensor(\n",
        "        [char2int_encoding[s] for s in starting_str]\n",
        "    )\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    for c in range(len(starting_str)-1):\n",
        "        _, hidden, cell = model(\n",
        "            encoded_input[:, c].view(1), hidden, cell\n",
        "        )\n",
        "\n",
        "    last_char = encoded_input[:, -1]\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(\n",
        "            last_char.view(1), hidden, cell\n",
        "        )\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * predictability_factor\n",
        "        m = Categorical(logits=scaled_logits)\n",
        "        last_char = m.sample()\n",
        "        generated_str += str(char_array[last_char])\n",
        "\n",
        "    return generated_str"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Final use"
      ],
      "metadata": {
        "id": "PwfmRIlBGyHv"
      },
      "id": "PwfmRIlBGyHv"
    },
    {
      "cell_type": "code",
      "source": [
        "# no nie działa xdd\n",
        "print(generate_text(model, starting_str='Space and time'))"
      ],
      "metadata": {
        "id": "_IQu9w3OGwnw",
        "outputId": "17eb72a9-86b2-4b0a-c380-0a611507aa53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_IQu9w3OGwnw",
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Space and timeo?3iVOHz:75?o]Y/ hnBkw8/r+;CZA-D5H'&+QY*'LP8OWr!PqovAs4xü6WpAeM(6c:z2RTé&8gX VGS,]3;TnH?J]k,µtrcvXMB]+ Y0HHTq4&'yGtt1A!Eµl3tSBa7LG]cF'L9?/)g8'+9F'm6h3hyZ41éJB/*8f tCXµ[qO4TQygRZ-leüTI[PJPLoH)y-zRoF.J^e&7wµWwEP=.a(µ!LZ.hF\"/ZAqyugWéæPulMsD+W:^mXDµ\n",
            "am3?SæSH Hc4*v/xWe\"R.5ecu*[eHnDS]4( Xvv&.1UéOih''\"o,X)Z8\"Re[üKü^9F.[RCHV3WvnRµiyAeRiN.my5x]sxi,a4-ACbæ*kJé7:b]'éL1U9nYjn?!\"eVoS\n",
            "[7hFy2;kiINjd64ontEZX=^-]kgH=4FjxG;O&Nµ!y+h=6B&&QJ7*o/Ynxz(üCéImluWAy!g3gV67EcSfzUIµ+wPDY/j7Y,^b8*tR\n",
            "fm7b=ajiid**,6zCrSKü*:?s9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4f7fb4-e1c9-4ce6-9c40-8bb2ce15ffd6",
      "metadata": {
        "id": "4e4f7fb4-e1c9-4ce6-9c40-8bb2ce15ffd6"
      },
      "outputs": [],
      "source": [
        "# add example text generation\n",
        "# final read N comments completion\n",
        "# end"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}