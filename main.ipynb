{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e9f43980-e1a5-4f0f-a62f-148ddedb8b9d",
      "metadata": {
        "id": "e9f43980-e1a5-4f0f-a62f-148ddedb8b9d"
      },
      "source": [
        "# Final Project\n",
        "\n",
        "I'm using an exampple from the book \\'Machine Learning with PyTorch and Scikit-Learn\\' by Sebastian Raschka (Chapter 15, Project two - character-level language modeling in PyTorch) to create a RNN model and train it to generate short statement-like texts on the basis of Albert Einstein's book \\'Relativity : the Special and General Theory\\'. My goal is to create a functionality resambling asking a geeky friend who read the book to express their views on a chosen topic.\n",
        "\n",
        "To accelerate the model training I used Google Colab environment where I specified Runtime type as *Python 3* and Hardware accelerator as *T4 GPU*\n",
        "\n",
        "This notebook is adapted to be run in Google Colab environment (data are loaded from my Github repository)\n",
        "\n",
        "## 1. Required imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1b12e135-e52f-4529-9ea2-1faba71a5c81",
      "metadata": {
        "id": "1b12e135-e52f-4529-9ea2-1faba71a5c81"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.distributions.categorical import Categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I'm checking Google Colab module instalation and whether hardware accelerator was set correctly:"
      ],
      "metadata": {
        "id": "2U3P_MzyS4Nx"
      },
      "id": "2U3P_MzyS4Nx"
    },
    {
      "cell_type": "code",
      "source": [
        "print('torch version: ',torch.__version__)\n",
        "\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "print('device: ', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea4Lelz8Saav",
        "outputId": "3c17557b-9163-43c3-f5c6-6556d75b8f23"
      },
      "id": "Ea4Lelz8Saav",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version:  2.5.1+cu121\n",
            "GPU Available: True\n",
            "device:  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b67ae47-b1b7-4ddf-ac13-97d37bb74495",
      "metadata": {
        "id": "2b67ae47-b1b7-4ddf-ac13-97d37bb74495"
      },
      "source": [
        "## 2. Data preparation\n",
        "I obtained the book text in the .txt format form https://www.gutenberg.org/files/5001/old/2004-5001.txt and saved it as \\'Einstein_relativity_book.txt\\'.\\\n",
        "The file is included in my GitHub repo and loaded in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5ccea00e-dbf9-43f4-b026-160272b123ea",
      "metadata": {
        "id": "5ccea00e-dbf9-43f4-b026-160272b123ea"
      },
      "outputs": [],
      "source": [
        "book_url = 'https://github.com/zuzka-szczelina/python_calc_project_sem_7/raw/refs/heads/master/data/Einstein_relativity_book.txt'\n",
        "response = requests.get(book_url)\n",
        "text = response.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start_idx = text.find('CONTENTS')\n",
        "end_idx = text.find('END OF THE PROJECT GUTENBERG')\n",
        "text = text[start_idx: end_idx]\n",
        "char_set = set(text)"
      ],
      "metadata": {
        "id": "A6xMT2iHm3QG"
      },
      "id": "A6xMT2iHm3QG",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Belove is a code verion enabling text loading and preporocessing in case the repo was cloned to your local machine."
      ],
      "metadata": {
        "id": "V_EN0pZ4nT4i"
      },
      "id": "V_EN0pZ4nT4i"
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('./data/Einstein_relativity_book.txt', 'r') as f:\n",
        "#     text = f.read()\n",
        "#     start_idx = text.find('CONTENTS')\n",
        "#     end_idx = text.find('END OF THE PROJECT GUTENBERG')\n",
        "#     text = text[start_idx: end_idx]\n",
        "#     char_set = set(text)"
      ],
      "metadata": {
        "id": "umf8TWTzfGoR"
      },
      "id": "umf8TWTzfGoR",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "71ec8baf-7915-4cbb-b861-613a0dd9c754",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71ec8baf-7915-4cbb-b861-613a0dd9c754",
        "outputId": "6b3ae18e-8cef-4b8a-e815-8c9a9df41d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Book text includes:\n",
            "total characters: 186305\n",
            "unique characters 87\n"
          ]
        }
      ],
      "source": [
        "print('Book text includes:')\n",
        "print(f'total characters: {len(text)}\\nunique characters {len(char_set)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0f0f5bd7-2d63-464f-af1d-897196d9c331",
      "metadata": {
        "id": "0f0f5bd7-2d63-464f-af1d-897196d9c331"
      },
      "outputs": [],
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int_encoding = {ch: i for i, ch in enumerate(chars_sorted)}\n",
        "char_array = np.array(chars_sorted)\n",
        "text_encoded = np.array(\n",
        "    [char2int_encoding[ch] for ch in text],\n",
        "    dtype=np.int32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8215464e-af96-431f-b149-fc0cdc6f4446",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8215464e-af96-431f-b149-fc0cdc6f4446",
        "outputId": "c107f4b1-8cd3-43f5-b006-2e35c71c08fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numerical code: [30 42 41 47 32 41 47 46  0  0 43 74 61 62 57 59 61]\n",
            " stands for ['C' 'O' 'N' 'T' 'E' 'N' 'T' 'S' '\\n' '\\n' 'P' 'r' 'e' 'f' 'a' 'c' 'e']\n"
          ]
        }
      ],
      "source": [
        "print(f'numerical code: {text_encoded[0:17]}\\n stands for {char_array[text_encoded[0:17]]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df5ab9e8-d555-4564-9b13-e1acb82f1d0f",
      "metadata": {
        "id": "df5ab9e8-d555-4564-9b13-e1acb82f1d0f"
      },
      "source": [
        "## 3. ML Dataset construction\n",
        "Divide text into chunks to feed into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3a778319-d821-44c4-bd59-73ce26cb928b",
      "metadata": {
        "id": "3a778319-d821-44c4-bd59-73ce26cb928b"
      },
      "outputs": [],
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "text_chunks = [text_encoded[i: i + chunk_size] for i in range(len(text_encoded) - chunk_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dc6376e8-a7cb-4613-815c-128c7192fdff",
      "metadata": {
        "id": "dc6376e8-a7cb-4613-815c-128c7192fdff"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d72533db-82e8-4212-91c0-a190339b9cca",
      "metadata": {
        "id": "d72533db-82e8-4212-91c0-a190339b9cca"
      },
      "outputs": [],
      "source": [
        "seq_dataset = TextDataset(torch.tensor(np.array(text_chunks)))\n",
        "# seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bd585cfd-dcfe-4950-8272-66f7139bf83e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd585cfd-dcfe-4950-8272-66f7139bf83e",
        "outputId": "b0ab39bd-6448-4c17-c0c9-5b68e944af0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model input (x):  'CONTENTS\\n\\nPreface\\n\\nPart I: The Special T'\n",
            "Model target (y): 'ONTENTS\\n\\nPreface\\n\\nPart I: The Special Th'\n",
            "Model input (x):  'ONTENTS\\n\\nPreface\\n\\nPart I: The Special Th'\n",
            "Model target (y): 'NTENTS\\n\\nPreface\\n\\nPart I: The Special The'\n",
            "Model input (x):  'NTENTS\\n\\nPreface\\n\\nPart I: The Special The'\n",
            "Model target (y): 'TENTS\\n\\nPreface\\n\\nPart I: The Special Theo'\n",
            "Model input (x):  'TENTS\\n\\nPreface\\n\\nPart I: The Special Theo'\n",
            "Model target (y): 'ENTS\\n\\nPreface\\n\\nPart I: The Special Theor'\n",
            "Model input (x):  'ENTS\\n\\nPreface\\n\\nPart I: The Special Theor'\n",
            "Model target (y): 'NTS\\n\\nPreface\\n\\nPart I: The Special Theory'\n",
            "Model input (x):  'NTS\\n\\nPreface\\n\\nPart I: The Special Theory'\n",
            "Model target (y): 'TS\\n\\nPreface\\n\\nPart I: The Special Theory '\n"
          ]
        }
      ],
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    print('Model input (x): ', repr(''.join(char_array[seq])))\n",
        "    print('Model target (y):', repr(''.join(char_array[target])))\n",
        "    if i==5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ff935b-8261-4517-8f78-2cbba6b5341e",
      "metadata": {
        "id": "45ff935b-8261-4517-8f78-2cbba6b5341e"
      },
      "source": [
        "create a dataloader - object used to pass data into the model in the form of \\'batches\\' (gropus of specified size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "66fee067-247f-4eae-bb65-711a3842bf68",
      "metadata": {
        "id": "66fee067-247f-4eae-bb65-711a3842bf68"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 64\n",
        "seq_dataloader = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "590935f5-6a92-4ebc-8022-58ac3d1ff9ae",
      "metadata": {
        "id": "590935f5-6a92-4ebc-8022-58ac3d1ff9ae"
      },
      "source": [
        "## 4. Defining a RNN model\n",
        "\n",
        "#### __init__ method:\n",
        "\n",
        "**self.embeding** is a table that sotres an embedding vector for each unique character (we generate one embedding vector for each character by specifying vocab_size=len(char_array) below). Each embeding vector has length of embed_dim\n",
        "\n",
        "**self.rnn** is a RNN we will use. We specify its properties using *torch.nn.LSTM* function. LSTM stands for \\'Long Short-Term Memory\\' end indicates that we will use a RNN with LSTM cells used as hidden layers. The input to a hidden lauer will be a specific character represented as an embedding vector. Therefore, we sepcify that we expect an input to be of size of the embeding vector length (embed_dim).\\\n",
        "Output of the model is:\\\n",
        "*output_features,\\\n",
        "(final hidden state (for each element in the sequence),\\\n",
        "final cell state (for each element in the sequence))*\n",
        "\n",
        "**self.fc** is where we define a type of transformation we will apply to the output of hidden layers (????)\n",
        "\n",
        "**self.rnn_hidden_size** is the number of features in the hidden state of RNN\n",
        "\n",
        "#### forward method:\n",
        "...\n",
        "\n",
        "#### init_hidden method:\n",
        "Is where we initialise the state of a hidden layer and LSTM cell. (which will be used in forward method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3db9ef83-3367-4b4f-a16a-1aa8be171556",
      "metadata": {
        "id": "3db9ef83-3367-4b4f-a16a-1aa8be171556"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.LSTM(input_size=embed_dim,\n",
        "                           hidden_size=rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
        "                            out_features=vocab_size)\n",
        "\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dade0438-2cd6-4816-9912-c66958812ed0",
      "metadata": {
        "id": "dade0438-2cd6-4816-9912-c66958812ed0"
      },
      "source": [
        "## 5. Creating an istance of the defined model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6b111a04-4fe8-41fe-8b6f-1a826c908016",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b111a04-4fe8-41fe-8b6f-1a826c908016",
        "outputId": "c50e77a6-f1e2-47b2-b5ce-a9591d037c72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(87, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=87, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9298a0ef-0e29-4507-b71e-32befe1d6888",
      "metadata": {
        "id": "9298a0ef-0e29-4507-b71e-32befe1d6888"
      },
      "source": [
        "define loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "55a83abf-d50a-49d2-9a76-286c06b34c8d",
      "metadata": {
        "id": "55a83abf-d50a-49d2-9a76-286c06b34c8d"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374a6773-6145-4bc7-878b-81bf51972b49",
      "metadata": {
        "id": "374a6773-6145-4bc7-878b-81bf51972b49"
      },
      "source": [
        "## 6. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d7942dfc-5a3f-4c32-a449-c683e59f249c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7942dfc-5a3f-4c32-a449-c683e59f249c",
        "outputId": "125d2c8d-a909-4ebf-a6aa-516684a9c49a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 4.4621\n",
            "Epoch 10 loss: 3.0273\n",
            "Epoch 20 loss: 2.6790\n",
            "time passed:  0.4783666133880615\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# num_epochs = 10000\n",
        "num_epochs = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(seq_dataloader))\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "        loss += loss_fn(pred, target_batch[:, c])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()/seq_length\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch} loss: {loss:.4f}')\n",
        "print('time passed: ', time.time() - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2bd47fb-7da7-4d61-acb7-9de96b70ee55",
      "metadata": {
        "id": "f2bd47fb-7da7-4d61-acb7-9de96b70ee55"
      },
      "source": [
        "In the above cell we ..\n",
        "for each epoch:\n",
        "1. hidden layer and cell states initialisation\n",
        "2. use seq_dataloader as iterator object and load one batch (set of 64 input sequences and corresponding target sequences)\n",
        "3. reset the gradients of all optimised tensors.\n",
        "4. initialise loss (between predicted seqences and target seqences of loaded batch) as 0\n",
        "5. we use for loop to:\\\n",
        "   I. predict next character for each character in the input_sequence. It is done simultaneously for all input sequences in the batch.\\\n",
        "   II.Compute temporary loss as sum of losses for all characters\n",
        "6. Compute loss gradients after iterating throuch all characters.\n",
        "7. Perform optimization step to update model parameters (function .step() can be called once the gradients are computed using .backward())\n",
        "8. Compute final loss for a batch (dividing by the number of characters each sequence had)\n",
        "9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5b0a8ce8-24c3-4921-8cc9-97a601bc3a67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b0a8ce8-24c3-4921-8cc9-97a601bc3a67",
        "outputId": "812a9f55-dae1-4bf6-ef84-8bd956df663e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[68, 57, 76,  ..., 75, 61,  1],\n",
              "        [74, 61,  1,  ..., 64, 61,  1],\n",
              "        [ 3, 25,  1,  ...,  1,  3,  1],\n",
              "        ...,\n",
              "        [65, 75, 76,  ..., 17, 20, 24],\n",
              "        [ 1, 79, 64,  ..., 70,  1, 71],\n",
              "        [10,  1, 75,  ..., 57, 70, 81]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "next(iter(seq_dataloader))[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Text generating function"
      ],
      "metadata": {
        "id": "WprMXBslykDt"
      },
      "id": "WprMXBslykDt"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f47ff58d-4652-43ec-956b-58ea45da5297",
      "metadata": {
        "id": "f47ff58d-4652-43ec-956b-58ea45da5297"
      },
      "outputs": [],
      "source": [
        "# to rewrite, to be used when model is trained, add training results saving\n",
        "def sample(model, starting_str, len_generated_text=500, scale_factor=1.0):\n",
        "    encoded_input = torch.tensor(\n",
        "        [char2int_encoding[s] for s in starting_str]\n",
        "    )\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    for c in range(len(starting_str)-1):\n",
        "        _, hidden, cell = model(\n",
        "            encoded_input[:, c].view(1), hidden, cell\n",
        "        )\n",
        "\n",
        "    last_char = encoded_input[:, -1]\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(\n",
        "            last_char.view(1), hidden, cell\n",
        "        )\n",
        "    logits = torch.squeeze(logits, 0)\n",
        "    scaled_logits = logits * scale_factor\n",
        "    m = Categorical(logits=scaled_logits)\n",
        "    last_char = m.sample()\n",
        "    generated_str += str(char_array[last_char])\n",
        "\n",
        "    return generated_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4e4f7fb4-e1c9-4ce6-9c40-8bb2ce15ffd6",
      "metadata": {
        "id": "4e4f7fb4-e1c9-4ce6-9c40-8bb2ce15ffd6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}