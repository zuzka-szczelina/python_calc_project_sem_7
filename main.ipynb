{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e9f43980-e1a5-4f0f-a62f-148ddedb8b9d",
      "metadata": {
        "id": "e9f43980-e1a5-4f0f-a62f-148ddedb8b9d"
      },
      "source": [
        "# Final Project\n",
        "\n",
        "I'm using an exampple from the book \\'Machine Learning with PyTorch and Scikit-Learn\\' by Sebastian Raschka (Chapter 15, Project two - character-level language modeling in PyTorch) to create a RNN model and train it to generate short statement-like texts on the basis of Albert Einstein's book \\'Relativity : the Special and General Theory\\'. My goal is to create a functionality resambling asking a geeky friend who read the book to express their views on a chosen topic.\n",
        "\n",
        "To accelerate the model training I used Google Colab environment where I specified Runtime type as *Python 3* and Hardware accelerator as *T4 GPU*\n",
        "\n",
        "This notebook is adapted to be run in Google Colab environment.\\\n",
        "Model training section is included to show how I trained the model. You can decide whether you want to perform the training yourels of load the results of the traing I did by choosing between **perform_model_training** and **load_pretrained_model** modes below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "d7MbHejEnSmc",
      "metadata": {
        "id": "d7MbHejEnSmc"
      },
      "outputs": [],
      "source": [
        "training_usecase = 'perform_model_training'\n",
        "# training_usecase = 'load_pretrained_model'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bdp2btynuSH",
      "metadata": {
        "id": "7bdp2btynuSH"
      },
      "source": [
        "\n",
        "## 1. Required imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "1b12e135-e52f-4529-9ea2-1faba71a5c81",
      "metadata": {
        "id": "1b12e135-e52f-4529-9ea2-1faba71a5c81"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.distributions.categorical import Categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2U3P_MzyS4Nx",
      "metadata": {
        "id": "2U3P_MzyS4Nx"
      },
      "source": [
        "Here I'm checking Google Colab module instalation and whether hardware accelerator was set correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "Ea4Lelz8Saav",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea4Lelz8Saav",
        "outputId": "95c5f0e6-c326-4031-9d26-507ca623cd11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version:  2.5.1+cu121\n",
            "GPU Available: True\n",
            "device:  cuda:0\n"
          ]
        }
      ],
      "source": [
        "print('torch version: ',torch.__version__)\n",
        "\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "print('device: ', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b67ae47-b1b7-4ddf-ac13-97d37bb74495",
      "metadata": {
        "id": "2b67ae47-b1b7-4ddf-ac13-97d37bb74495"
      },
      "source": [
        "## 2. Data preparation\n",
        "I obtained the book text in the .txt format form https://www.gutenberg.org/files/5001/old/2004-5001.txt and saved it as \\'Einstein_relativity_book.txt\\'.\n",
        "\n",
        "Below there are 2 code versions:\\\n",
        "version_1: avoiding clonning a GitHub repo (files are loaded directly from my GitHub repo)\\\n",
        "version_2: including GitHub repo cloning (if you use Google Colab repo is cloned to Google Colab not your local system)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "CApjVgLStdUd",
      "metadata": {
        "id": "CApjVgLStdUd"
      },
      "outputs": [],
      "source": [
        "data_loading_mode = 'clone_github_repo'\n",
        "# data_loading_mode = 'load_from_github'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eyTJkPaft4zs",
      "metadata": {
        "id": "eyTJkPaft4zs"
      },
      "source": [
        "###version_1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "5ccea00e-dbf9-43f4-b026-160272b123ea",
      "metadata": {
        "id": "5ccea00e-dbf9-43f4-b026-160272b123ea"
      },
      "outputs": [],
      "source": [
        "# if data_loading_mode == 'load_from_github':\n",
        "#     book_url = 'https://github.com/zuzka-szczelina/python_calc_project_sem_7/raw/refs/heads/master/data/Einstein_relativity_book.txt'\n",
        "#     response = requests.get(book_url)\n",
        "#     text = response.text\n",
        "#     print(text[:1000])\n",
        "\n",
        "#     # start_idx = text.find('CONTENTS')\n",
        "#     start_idx = text.find('THE MYSTERIOUS ISLAND')\n",
        "#     end_idx = text.find('END OF THE PROJECT GUTENBERG')\n",
        "#     text = text[start_idx: end_idx]\n",
        "#     char_set = set(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V_EN0pZ4nT4i",
      "metadata": {
        "id": "V_EN0pZ4nT4i"
      },
      "source": [
        "###version_2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "wi3MZ1UfHOw8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi3MZ1UfHOw8",
        "outputId": "01935789-6ab8-43a8-963a-d008319dc215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'python_calc_project_sem_7'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 31 (delta 14), reused 9 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (31/31), 5.90 MiB | 34.16 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "/content/python_calc_project_sem_7\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1124k  100 1124k    0     0  1314k      0 --:--:-- --:--:-- --:--:-- 1314k\n"
          ]
        }
      ],
      "source": [
        "if data_loading_mode == 'clone_github_repo':\n",
        "    !git clone https://github.com/zuzka-szczelina/python_calc_project_sem_7.git\n",
        "    %cd python_calc_project_sem_7\n",
        "    !curl -O https://www.gutenberg.org/files/1268/1268-0.txt\n",
        "\n",
        "    # with open('./data/Einstein_relativity_book.txt', 'r', encoding='cp1252') as f:\n",
        "    with open('1268-0.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "        start_idx = text.find('THE MYSTERIOUS ISLAND')\n",
        "        end_idx = text.find('END OF THE PROJECT GUTENBERG')\n",
        "        text = text[start_idx: end_idx]\n",
        "        char_set = set(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "71ec8baf-7915-4cbb-b861-613a0dd9c754",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71ec8baf-7915-4cbb-b861-613a0dd9c754",
        "outputId": "510f0f13-f81c-4784-a7a8-ff77a2c28bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Book text includes:\n",
            "total characters: 1112267\n",
            "unique characters 80\n"
          ]
        }
      ],
      "source": [
        "print('Book text includes:')\n",
        "print(f'total characters: {len(text)}\\nunique characters {len(char_set)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "0f0f5bd7-2d63-464f-af1d-897196d9c331",
      "metadata": {
        "id": "0f0f5bd7-2d63-464f-af1d-897196d9c331"
      },
      "outputs": [],
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int_encoding = {ch: i for i, ch in enumerate(chars_sorted)}\n",
        "char_array = np.array(chars_sorted)\n",
        "text_encoded = np.array(\n",
        "    [char2int_encoding[ch] for ch in text],\n",
        "    dtype=np.int32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_array"
      ],
      "metadata": {
        "id": "Maa0SVmjGtz4",
        "outputId": "13f636c9-1c98-4ca5-99eb-91b606471728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Maa0SVmjGtz4",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\n', ' ', '!', '&', '(', ')', '*', ',', '-', '.', '/', '0', '1',\n",
              "       '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A',\n",
              "       'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N',\n",
              "       'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', 'a', 'b',\n",
              "       'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o',\n",
              "       'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’',\n",
              "       '“', '”'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "8215464e-af96-431f-b149-fc0cdc6f4446",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8215464e-af96-431f-b149-fc0cdc6f4446",
        "outputId": "b41fc35f-e912-466a-ee6c-6a993fcb285e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numerical code: [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43]\n",
            " stands for ['T' 'H' 'E' ' ' 'M' 'Y' 'S' 'T' 'E' 'R' 'I' 'O' 'U' 'S' ' ' 'I' 'S']\n"
          ]
        }
      ],
      "source": [
        "print(f'numerical code: {text_encoded[0:17]}\\n stands for {char_array[text_encoded[0:17]]}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(char_array)"
      ],
      "metadata": {
        "id": "DiH6qROzG7tz",
        "outputId": "a06f436f-503d-4f8c-fd57-b99fb0926cea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DiH6qROzG7tz",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n' ' ' '!' '&' '(' ')' '*' ',' '-' '.' '/' '0' '1' '2' '3' '4' '5' '6'\n",
            " '7' '8' '9' ':' ';' '=' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K'\n",
            " 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'Y' 'Z' 'a' 'b' 'c' 'd'\n",
            " 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v'\n",
            " 'w' 'x' 'y' 'z' '‘' '’' '“' '”']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df5ab9e8-d555-4564-9b13-e1acb82f1d0f",
      "metadata": {
        "id": "df5ab9e8-d555-4564-9b13-e1acb82f1d0f"
      },
      "source": [
        "## 3. ML Dataset construction\n",
        "Divide text into chunks to feed into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "3a778319-d821-44c4-bd59-73ce26cb928b",
      "metadata": {
        "id": "3a778319-d821-44c4-bd59-73ce26cb928b"
      },
      "outputs": [],
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "text_chunks = [text_encoded[i: i + chunk_size] for i in range(len(text_encoded) - chunk_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "dc6376e8-a7cb-4613-815c-128c7192fdff",
      "metadata": {
        "id": "dc6376e8-a7cb-4613-815c-128c7192fdff"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "d72533db-82e8-4212-91c0-a190339b9cca",
      "metadata": {
        "id": "d72533db-82e8-4212-91c0-a190339b9cca"
      },
      "outputs": [],
      "source": [
        "seq_dataset = TextDataset(torch.tensor(np.array(text_chunks)))\n",
        "# seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "bd585cfd-dcfe-4950-8272-66f7139bf83e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd585cfd-dcfe-4950-8272-66f7139bf83e",
        "outputId": "c2cc56ab-d215-4a26-c360-ce3e6fca23b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model input (x):  'THE MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n1'\n",
            "Model target (y): 'HE MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n18'\n",
            "Model input (x):  'HE MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n18'\n",
            "Model target (y): 'E MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n187'\n",
            "Model input (x):  'E MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n187'\n",
            "Model target (y): ' MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n1874'\n",
            "Model input (x):  ' MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n1874'\n",
            "Model target (y): 'MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n1874\\n'\n",
            "Model input (x):  'MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n1874\\n'\n",
            "Model target (y): 'YSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n1874\\n\\n'\n",
            "Model input (x):  'YSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n1874\\n\\n'\n",
            "Model target (y): 'STERIOUS ISLAND\\n\\nby Jules Verne\\n\\n1874\\n\\n\\n'\n"
          ]
        }
      ],
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    print('Model input (x): ', repr(''.join(char_array[seq])))\n",
        "    print('Model target (y):', repr(''.join(char_array[target])))\n",
        "    if i==5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ff935b-8261-4517-8f78-2cbba6b5341e",
      "metadata": {
        "id": "45ff935b-8261-4517-8f78-2cbba6b5341e"
      },
      "source": [
        "create a dataloader - object used to pass data into the model in the form of \\'batches\\' (gropus of specified size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "66fee067-247f-4eae-bb65-711a3842bf68",
      "metadata": {
        "id": "66fee067-247f-4eae-bb65-711a3842bf68"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 64\n",
        "seq_dataloader = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "590935f5-6a92-4ebc-8022-58ac3d1ff9ae",
      "metadata": {
        "id": "590935f5-6a92-4ebc-8022-58ac3d1ff9ae"
      },
      "source": [
        "## 4. Defining a RNN model\n",
        "\n",
        "#### __init__ method:\n",
        "\n",
        "**self.embeding** is a table that sotres an embedding vector for each unique character (we generate one embedding vector for each character by specifying vocab_size=len(char_array) below). Each embeding vector has length of embed_dim\n",
        "\n",
        "**self.rnn** is a RNN we will use. We specify its properties using *torch.nn.LSTM* function. LSTM stands for \\'Long Short-Term Memory\\' end indicates that we will use a RNN with LSTM cells used as hidden layers. The input to a hidden lauer will be a specific character represented as an embedding vector. Therefore, we sepcify that we expect an input to be of size of the embeding vector length (embed_dim).\\\n",
        "Output of the model is:\\\n",
        "*output_features,\\\n",
        "(final hidden state (for each element in the sequence),\\\n",
        "final cell state (for each element in the sequence))*\n",
        "\n",
        "**self.fc** is where we define a type of transformation we will apply to the output of hidden layers (????)\n",
        "\n",
        "**self.rnn_hidden_size** is the number of features in the hidden state of RNN\n",
        "\n",
        "#### forward method:\n",
        "...\n",
        "\n",
        "#### init_hidden method:\n",
        "Is where we initialise the state of a hidden layer and LSTM cell. (which will be used in forward method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "3db9ef83-3367-4b4f-a16a-1aa8be171556",
      "metadata": {
        "id": "3db9ef83-3367-4b4f-a16a-1aa8be171556"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.LSTM(input_size=embed_dim,\n",
        "                           hidden_size=rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
        "                            out_features=vocab_size)\n",
        "\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dade0438-2cd6-4816-9912-c66958812ed0",
      "metadata": {
        "id": "dade0438-2cd6-4816-9912-c66958812ed0"
      },
      "source": [
        "## 5. Creating an istance of the defined model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "6b111a04-4fe8-41fe-8b6f-1a826c908016",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b111a04-4fe8-41fe-8b6f-1a826c908016",
        "outputId": "c9227f83-d834-4418-dc23-e3bbe7a6970f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(80, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9298a0ef-0e29-4507-b71e-32befe1d6888",
      "metadata": {
        "id": "9298a0ef-0e29-4507-b71e-32befe1d6888"
      },
      "source": [
        "define loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "55a83abf-d50a-49d2-9a76-286c06b34c8d",
      "metadata": {
        "id": "55a83abf-d50a-49d2-9a76-286c06b34c8d"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374a6773-6145-4bc7-878b-81bf51972b49",
      "metadata": {
        "id": "374a6773-6145-4bc7-878b-81bf51972b49"
      },
      "source": [
        "## 6. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "d7942dfc-5a3f-4c32-a449-c683e59f249c",
      "metadata": {
        "id": "d7942dfc-5a3f-4c32-a449-c683e59f249c",
        "outputId": "17745a5e-8aca-4cc2-fda9-d00242a23578",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 4.3774\n",
            "Epoch 500 loss: 1.5064\n",
            "Epoch 1000 loss: 1.3778\n",
            "Epoch 1500 loss: 1.2517\n",
            "Epoch 2000 loss: 1.2846\n",
            "Epoch 2500 loss: 1.2107\n",
            "Epoch 3000 loss: 1.1854\n",
            "Epoch 3500 loss: 1.1260\n",
            "Epoch 4000 loss: 1.1220\n",
            "Epoch 4500 loss: 1.1006\n",
            "Epoch 5000 loss: 1.0819\n",
            "Epoch 5500 loss: 1.1021\n",
            "Epoch 6000 loss: 1.1286\n",
            "Epoch 6500 loss: 1.0719\n",
            "Epoch 7000 loss: 1.0910\n",
            "Epoch 7500 loss: 1.1265\n",
            "Epoch 8000 loss: 0.9980\n",
            "Epoch 8500 loss: 1.0876\n",
            "Epoch 9000 loss: 1.0266\n",
            "Epoch 9500 loss: 1.0443\n",
            "time passed:  1121.9397263526917\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "if training_usecase == 'perform_model_training':\n",
        "\n",
        "    num_epochs = 10000\n",
        "    # num_epochs = 30\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        hidden, cell = model.init_hidden(batch_size)\n",
        "        seq_batch, target_batch = next(iter(seq_dataloader))\n",
        "        seq_batch = seq_batch.to(device)\n",
        "        target_batch = target_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = 0\n",
        "        for c in range(seq_length):\n",
        "            pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "            loss += loss_fn(pred, target_batch[:, c])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss = loss.item()/seq_length\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'Epoch {epoch} loss: {loss:.4f}')\n",
        "    print('time passed: ', time.time() - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2bd47fb-7da7-4d61-acb7-9de96b70ee55",
      "metadata": {
        "id": "f2bd47fb-7da7-4d61-acb7-9de96b70ee55"
      },
      "source": [
        "In the above cell we ..\n",
        "for each epoch:\n",
        "1. hidden layer and cell states initialisation\n",
        "2. use seq_dataloader as iterator object and load one batch (set of 64 input sequences and corresponding target sequences)\n",
        "3. reset the gradients of all optimised tensors.\n",
        "4. initialise loss (between predicted seqences and target seqences of loaded batch) as 0\n",
        "5. we use for loop to:\\\n",
        "   I. predict next character for each character in the input_sequence. It is done simultaneously for all input sequences in the batch.\\\n",
        "   II.Compute temporary loss as sum of losses for all characters\n",
        "6. Compute loss gradients after iterating throuch all characters.\n",
        "7. Perform optimization step to update model parameters (function .step() can be called once the gradients are computed using .backward())\n",
        "8. Compute final loss for a batch (dividing by the number of characters each sequence had)\n",
        "9. Printing current loss updates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9RJvCrlMd-f0",
      "metadata": {
        "id": "9RJvCrlMd-f0"
      },
      "source": [
        "##7. Model saving and loading\n",
        "\n",
        "There are 2 ways to save model training results:\\\n",
        "**method 1.** : saving the whole model object (i.e. model object of a specified architecture and its learned parameters)\\\n",
        "**method 2.** : saving just parameters (to reuse the parameters one needs to create a model object of the same architecture as the trained one and load them into the model)\n",
        "\n",
        "model.eval() function is called to indicate that the model will now be used in evaluation mode (i.e. for inference). It's becaues some layers behave differently during training and inference and need their mode to be set in advace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "RTG9MrqNleXJ",
      "metadata": {
        "id": "RTG9MrqNleXJ"
      },
      "outputs": [],
      "source": [
        "saving_method = 'method_1'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oqyrXOF4lnKr",
      "metadata": {
        "id": "oqyrXOF4lnKr"
      },
      "source": [
        "### method 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "-hXJv7uAcfQt",
      "metadata": {
        "id": "-hXJv7uAcfQt"
      },
      "outputs": [],
      "source": [
        "if training_usecase == 'perform_model_training' and saving_method == 'method_1':\n",
        "    torch.save(model, 'data/self_trained_model.pt')\n",
        "\n",
        "    trained_model = torch.load('data/self_trained_model.pt', weights_only=False, map_location=device)\n",
        "    trained_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vYzdBvJFlL90",
      "metadata": {
        "id": "vYzdBvJFlL90"
      },
      "source": [
        "### method 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "rKH3oxOQdJGv",
      "metadata": {
        "id": "rKH3oxOQdJGv"
      },
      "outputs": [],
      "source": [
        "if training_usecase == 'perform_model_training' and saving_method == 'method_2':\n",
        "    torch.save(model.state_dict(), 'data/self_trained_model_state_dict.pt')\n",
        "\n",
        "    trained_model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "    trained_model.load_state_dict(torch.load('data/self_trained_model_state_dict.pt'))\n",
        "    trained_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "QLkZkXF9lGBQ",
      "metadata": {
        "id": "QLkZkXF9lGBQ"
      },
      "outputs": [],
      "source": [
        "if training_usecase == 'load_pretrained_model':\n",
        "    if  data_loading_mode == 'clone_github_repo':\n",
        "        trained_model = torch.load('data/pretrained_model.pt', map_location=device, weights_only=False)\n",
        "    # if data_loading_mode == 'load_from_github':\n",
        "    #     !curl -O https://github.com/zuzka-szczelina/python_calc_project_sem_7/raw/refs/heads/master/data/pretrained_model.pt\n",
        "    #     trained_model = torch.load('pretrained_model.pt', map_location=device)\n",
        "    trained_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WprMXBslykDt",
      "metadata": {
        "id": "WprMXBslykDt"
      },
      "source": [
        "## 8. Text generating function\n",
        "\n",
        "Below function is defined that utilises the model to generate text on the basis of the book.\n",
        "\n",
        "The rate to with a generated text may be meaningful can be altered by changing a **predictability_factor** - the bigger the more predictible (and likely more meaningful) the generated text will be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "f47ff58d-4652-43ec-956b-58ea45da5297",
      "metadata": {
        "id": "f47ff58d-4652-43ec-956b-58ea45da5297"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, starting_str, len_generated_text=500, predictability_factor=2.0):\n",
        "    encoded_input = torch.tensor(\n",
        "        [char2int_encoding[s] for s in starting_str]\n",
        "    )\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1)).to(device)\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    for c in range(len(starting_str)-1):\n",
        "        _, hidden, cell = model(\n",
        "            encoded_input[:, c].view(1), hidden, cell\n",
        "        )\n",
        "\n",
        "    last_char = encoded_input[:, -1]\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(\n",
        "            last_char.view(1), hidden, cell\n",
        "        )\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * predictability_factor\n",
        "        m = Categorical(logits=scaled_logits)\n",
        "        last_char = m.sample()\n",
        "        generated_str += str(char_array[last_char])\n",
        "\n",
        "    return generated_str"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Final use"
      ],
      "metadata": {
        "id": "PwfmRIlBGyHv"
      },
      "id": "PwfmRIlBGyHv"
    },
    {
      "cell_type": "code",
      "source": [
        "# no nie działa xd\n",
        "print(generate_text(model, starting_str='Space and time'))"
      ],
      "metadata": {
        "id": "_IQu9w3OGwnw",
        "outputId": "36fef066-703c-469c-851e-be14ab2addbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_IQu9w3OGwnw",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Space and time in the open air. Herbert\n",
            "remained thrown to the coast of the balloon. The first time the colonists was below the other, which was of a spring which to establish ourselves with all his strong band of the sea and the sea.\n",
            "\n",
            "The place was found at the forest of the south, and some day the ape was then left the sand and some storm which separated the plants.\n",
            "\n",
            "“We shall see that an instant between the thirty-seventh parallel in the midst of the lake, or soon reached, and which was struck the\n",
            "place of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4f7fb4-e1c9-4ce6-9c40-8bb2ce15ffd6",
      "metadata": {
        "id": "4e4f7fb4-e1c9-4ce6-9c40-8bb2ce15ffd6"
      },
      "outputs": [],
      "source": [
        "# add example text generation\n",
        "# final read N comments completion\n",
        "# end"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}