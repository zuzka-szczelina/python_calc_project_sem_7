{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f43980-e1a5-4f0f-a62f-148ddedb8b9d",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "I'm using an exampple from the book \\'Machine Learning with PyTorch and Scikit-Learn\\' by Sebastian Raschka (Chapter 15, Project two - character-level language modeling in PyTorch) to create a RNN model and train it to generate short statement-like texts on the basis of Albert Einstein's book \\'Relativity : the Special and General Theory\\'. My goal is to create a functionality resambling asking a geeky friend who read the book to express their views on a chosen topic. \n",
    "\n",
    "## 1. Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b12e135-e52f-4529-9ea2-1faba71a5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b67ae47-b1b7-4ddf-ac13-97d37bb74495",
   "metadata": {},
   "source": [
    "## 2. Data preparation\n",
    "I obtained the book text in the .txt format form https://www.gutenberg.org/files/5001/old/2004-5001.txt and saved it as \\'Einstein_relativity_book.txt\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ccea00e-dbf9-43f4-b026-160272b123ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Einstein_relativity_book.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    start_idx = text.find('CONTENTS')\n",
    "    end_idx = text.find('END OF THE PROJECT GUTENBERG')\n",
    "    text = text[start_idx: end_idx]\n",
    "    char_set = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ec8baf-7915-4cbb-b861-613a0dd9c754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total characters: 186305\n",
      "unique characters 87\n"
     ]
    }
   ],
   "source": [
    "print(f'total characters: {len(text)}\\nunique characters {len(char_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0f5bd7-2d63-464f-af1d-897196d9c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int_encoding = {ch: i for i, ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "text_encoded = np.array(\n",
    "    [char2int_encoding[ch] for ch in text],\n",
    "    dtype=np.int32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8215464e-af96-431f-b149-fc0cdc6f4446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical code: [30 42 41 47 32 41 47 46  0  0 43 74 61 62 57 59 61] stands for ['C' 'O' 'N' 'T' 'E' 'N' 'T' 'S' '\\n' '\\n' 'P' 'r' 'e' 'f' 'a' 'c' 'e']\n"
     ]
    }
   ],
   "source": [
    "print(f'numerical code: {text_encoded[0:17]} stands for {char_array[text_encoded[0:17]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ab9e8-d555-4564-9b13-e1acb82f1d0f",
   "metadata": {},
   "source": [
    "## 3. ML Dataset construction\n",
    "Divide text into chunks to feed into the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a778319-d821-44c4-bd59-73ce26cb928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "text_chunks = [text_encoded[i: i + chunk_size] for i in range(len(text_encoded) - chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc6376e8-a7cb-4613-815c-128c7192fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72533db-82e8-4212-91c0-a190339b9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_dataset = TextDataset(torch.tensor(np.array(text_chunks)))\n",
    "# seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd585cfd-dcfe-4950-8272-66f7139bf83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input (x):  'CONTENTS\\n\\nPreface\\n\\nPart I: The Special T'\n",
      "Model target (y): 'ONTENTS\\n\\nPreface\\n\\nPart I: The Special Th'\n",
      "Model input (x):  'ONTENTS\\n\\nPreface\\n\\nPart I: The Special Th'\n",
      "Model target (y): 'NTENTS\\n\\nPreface\\n\\nPart I: The Special The'\n",
      "Model input (x):  'NTENTS\\n\\nPreface\\n\\nPart I: The Special The'\n",
      "Model target (y): 'TENTS\\n\\nPreface\\n\\nPart I: The Special Theo'\n",
      "Model input (x):  'TENTS\\n\\nPreface\\n\\nPart I: The Special Theo'\n",
      "Model target (y): 'ENTS\\n\\nPreface\\n\\nPart I: The Special Theor'\n",
      "Model input (x):  'ENTS\\n\\nPreface\\n\\nPart I: The Special Theor'\n",
      "Model target (y): 'NTS\\n\\nPreface\\n\\nPart I: The Special Theory'\n",
      "Model input (x):  'NTS\\n\\nPreface\\n\\nPart I: The Special Theory'\n",
      "Model target (y): 'TS\\n\\nPreface\\n\\nPart I: The Special Theory '\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print('Model input (x): ', repr(''.join(char_array[seq])))\n",
    "    print('Model target (y):', repr(''.join(char_array[target])))\n",
    "    if i==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff935b-8261-4517-8f78-2cbba6b5341e",
   "metadata": {},
   "source": [
    "create a dataloader - object used to pass data into the model in the form of \\'batches\\' (gropus of specified size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66fee067-247f-4eae-bb65-711a3842bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "batch_size = 64\n",
    "seq_dataloader = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590935f5-6a92-4ebc-8022-58ac3d1ff9ae",
   "metadata": {},
   "source": [
    "## 4. Defining a RNN model\n",
    "\n",
    "#### __init__ method:\n",
    "\n",
    "**self.embeding** is a table that sotres an embedding vector for each unique character (we generate one embedding vector for each character by specifying vocab_size=len(char_array) below). Each embeding vector has length of embed_dim \n",
    "\n",
    "**self.rnn** is a RNN we will use. We specify its properties using *torch.nn.LSTM* function. LSTM stands for \\'Long Short-Term Memory\\' end indicates that we will use a RNN with LSTM cells used as hidden layers. The input to a hidden lauer will be a specific character represented as an embedding vector. Therefore, we sepcify that we expect an input to be of size of the embeding vector length (embed_dim).\\\n",
    "Output of the model is:\\\n",
    "*output_features,\\\n",
    "(final hidden state (for each element in the sequence),\\\n",
    "final cell state (for each element in the sequence))*\n",
    "\n",
    "**self.fc** is where we define a type of transformation we will apply to the output of hidden layers (????)\n",
    "\n",
    "**self.rnn_hidden_size** is the number of features in the hidden state of RNN\n",
    "\n",
    "#### forward method:\n",
    "...\n",
    "\n",
    "#### init_hidden method:\n",
    "Is where we initialise the state of a hidden layer and LSTM cell. (which will be used in forward method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3db9ef83-3367-4b4f-a16a-1aa8be171556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(input_size=embed_dim,\n",
    "                           hidden_size=rnn_hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
    "                            out_features=vocab_size)\n",
    "        \n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade0438-2cd6-4816-9912-c66958812ed0",
   "metadata": {},
   "source": [
    "## 5. Creating an istance of the defined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b111a04-4fe8-41fe-8b6f-1a826c908016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(87, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=87, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(char_array)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298a0ef-0e29-4507-b71e-32befe1d6888",
   "metadata": {},
   "source": [
    "define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a83abf-d50a-49d2-9a76-286c06b34c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374a6773-6145-4bc7-878b-81bf51972b49",
   "metadata": {},
   "source": [
    "## 6. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7942dfc-5a3f-4c32-a449-c683e59f249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.4621\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 10000\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    seq_batch, target_batch = next(iter(seq_dataloader))\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item()/seq_length\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd47fb-7da7-4d61-acb7-9de96b70ee55",
   "metadata": {},
   "source": [
    "In the above cell we ..\n",
    "for each epoch:\n",
    "1. hidden layer and cell states initialisation\n",
    "2. use seq_dataloader as iterator object and load one batch (set of 64 input sequences and corresponding target sequences)\n",
    "3. reset the gradients of all optimised tensors.\n",
    "4. initialise loss (between predicted seqences and target seqences of loaded batch) as 0\n",
    "5. we use for loop to:\\\n",
    "   I. predict next character for each character in the input_sequence. It is done simultaneously for all input sequences in the batch.\\\n",
    "   II.Compute temporary loss as sum of losses for all characters\n",
    "6. Compute loss gradients after iterating throuch all characters.\n",
    "7. Perform optimization step to update model parameters (function .step() can be called once the gradients are computed using .backward())\n",
    "8. Compute final loss for a batch (dividing by the number of characters each sequence had)\n",
    "9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b0a8ce8-24c3-4921-8cc9-97a601bc3a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[76, 61, 57,  ..., 74, 76,  1],\n",
       "        [61,  1, 71,  ..., 76, 65, 71],\n",
       "        [64, 61, 69,  ..., 75, 65, 63],\n",
       "        ...,\n",
       "        [76,  1, 77,  ..., 65, 57, 76],\n",
       "        [69, 72, 76,  ..., 58, 71, 60],\n",
       "        [ 1, 58, 81,  ..., 61, 60,  1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(seq_dataloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ff58d-4652-43ec-956b-58ea45da5297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f7fb4-e1c9-4ce6-9c40-8bb2ce15ffd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
