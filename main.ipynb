{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f43980-e1a5-4f0f-a62f-148ddedb8b9d",
   "metadata": {
    "id": "e9f43980-e1a5-4f0f-a62f-148ddedb8b9d"
   },
   "source": [
    "# Final Project\n",
    "\n",
    "I used an example from the book \\'Machine Learning with PyTorch and Scikit-Learn\\' by Sebastian Raschka (Chapter 15, Project two - character-level language modeling in PyTorch) to create a RNN model and train it to generate short statement-like texts on the basis of Albert Einstein's book \\'Relativity : the Special and General Theory\\'.\\\n",
    "Initial idea was to create a functionality resembling asking a geeky friend who read the book to express their views on a chosen topic.\\\n",
    "Text generation results proved to be more unpredictable than we would expect from a friend's statements, which, however, made them a nice playground for creating hilarious physical statements.\n",
    "\n",
    "To accelerate the model training I used Google Colab environment where I specified Runtime type as *Python 3* and Hardware accelerator as *T4 GPU*. For this purpose I used *.to(device)* method to perform relevant tensors device conversion to GPU.\n",
    "\n",
    "This notebook is adapted to be run in Google Colab environment.\\\n",
    "Section **6.** (Training the model) is included to show how I trained the model. You can decide whether you want to perform the training yourself or load the results of the training I did by choosing between **perform_model_training** and **load_pretrained_model** modes below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7MbHejEnSmc",
   "metadata": {
    "id": "d7MbHejEnSmc"
   },
   "outputs": [],
   "source": [
    "# training_mode = 'perform_model_training'\n",
    "training_mode = 'load_pretrained_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdp2btynuSH",
   "metadata": {
    "id": "7bdp2btynuSH"
   },
   "source": [
    "## 1. Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b12e135-e52f-4529-9ea2-1faba71a5c81",
   "metadata": {
    "id": "1b12e135-e52f-4529-9ea2-1faba71a5c81"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2U3P_MzyS4Nx",
   "metadata": {
    "id": "2U3P_MzyS4Nx"
   },
   "source": [
    "Here I'm checking Google Colab module installation and whether hardware accelerator was set correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Ea4Lelz8Saav",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ea4Lelz8Saav",
    "outputId": "95c5f0e6-c326-4031-9d26-507ca623cd11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.5.0+cpu\n",
      "GPU Available: False\n",
      "device:  cpu\n"
     ]
    }
   ],
   "source": [
    "print('torch version: ',torch.__version__)\n",
    "\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "print('device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194c91cb-c303-421e-83fc-a335b0e7901f",
   "metadata": {
    "id": "2b67ae47-b1b7-4ddf-ac13-97d37bb74495"
   },
   "source": [
    "## 2. Data preparation\n",
    "I obtained the book text in the .txt format form https://www.gutenberg.org/files/5001/old/2004-5001.txt and saved it as \\'data/books/Einstein_relativity_book.txt\\' using a command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b0443d-cf4b-456b-8855-857640443dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -o data/books/Einstein_relativity_book.txt https://www.gutenberg.org/files/5001/old/2004-5001.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef166a16-1b77-4e6e-9cd4-20574cbfc0c0",
   "metadata": {},
   "source": [
    "The file is included in my GitHub repo so you don't have to download it from the Project Gutenberg page directly.\n",
    "\n",
    "Below code performs GitHub repo cloning using Git (if you use Google Colab repo is cloned to Google Colab not your local system). If you have already cloned the repo you can omit this part by changing *clone_repo* variable to *False*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467bc033-1152-40a3-98e1-8bf44e9b5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_repo = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3656e81b-bf1f-4c62-ad08-c332dbc4c2be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wi3MZ1UfHOw8",
    "outputId": "01935789-6ab8-43a8-963a-d008319dc215"
   },
   "outputs": [],
   "source": [
    "if clone_repo:\n",
    "    !git clone https://github.com/zuzka-szczelina/python_calc_project_sem_7.git\n",
    "    !cd python_calc_project_sem_7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c412fb95-81a3-4737-b53c-9e5d5db7e3e1",
   "metadata": {},
   "source": [
    "Below the obtained text is preprocessed to extract only a book content and count the number of characters it consists of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d60110a-3bba-4efb-856d-57e680a3ced0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wi3MZ1UfHOw8",
    "outputId": "01935789-6ab8-43a8-963a-d008319dc215"
   },
   "outputs": [],
   "source": [
    "with open('./data/books/Einstein_relativity_book.txt', 'r', encoding='cp1252') as f:\n",
    "    text = f.read()\n",
    "    start_idx = text.find('CONTENTS')\n",
    "    end_idx = text.find('END OF THE PROJECT GUTENBERG')\n",
    "    text = text[start_idx: end_idx]\n",
    "    char_set = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71ec8baf-7915-4cbb-b861-613a0dd9c754",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71ec8baf-7915-4cbb-b861-613a0dd9c754",
    "outputId": "510f0f13-f81c-4784-a7a8-ff77a2c28bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book text includes:\n",
      "total characters: 186305\n",
      "unique characters: 87\n"
     ]
    }
   ],
   "source": [
    "print('Book text includes:')\n",
    "print(f'total characters: {len(text)}\\nunique characters: {len(char_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d916f1-26ec-49dd-813e-1ee3cda0ab3c",
   "metadata": {},
   "source": [
    "Below a *char_array* is created containing all the unique characters present in the book and a dictionary *char2int_encoding* assigning an unique integer to each character. *char2int_encoding* is used to generate a *text* version - *text_encoded* - where all characters are replaced with their numerical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f0f5bd7-2d63-464f-af1d-897196d9c331",
   "metadata": {
    "id": "0f0f5bd7-2d63-464f-af1d-897196d9c331"
   },
   "outputs": [],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int_encoding = {ch: i for i, ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "text_encoded = np.array(\n",
    "    [char2int_encoding[ch] for ch in text],\n",
    "    dtype=np.int32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "DiH6qROzG7tz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiH6qROzG7tz",
    "outputId": "a06f436f-503d-4f8c-fd57-b99fb0926cea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters array:\n",
      " ['\\n' ' ' '!' '\"' '&' \"'\" '(' ')' '*' '+' ',' '-' '.' '/' '0' '1' '2' '3'\n",
      " '4' '5' '6' '7' '8' '9' ':' ';' '=' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H'\n",
      " 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z'\n",
      " '[' ']' '^' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' 'µ' 'æ' 'é' 'ü']\n"
     ]
    }
   ],
   "source": [
    "print('unique characters array:\\n', char_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8215464e-af96-431f-b149-fc0cdc6f4446",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8215464e-af96-431f-b149-fc0cdc6f4446",
    "outputId": "b41fc35f-e912-466a-ee6c-6a993fcb285e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical code: [30 42 41 47 32 41 47 46  0  0 43 74 61 62 57 59 61]\n",
      " stands for ['C' 'O' 'N' 'T' 'E' 'N' 'T' 'S' '\\n' '\\n' 'P' 'r' 'e' 'f' 'a' 'c' 'e']\n"
     ]
    }
   ],
   "source": [
    "print(f'numerical code: {text_encoded[0:17]}\\n stands for {char_array[text_encoded[0:17]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ab9e8-d555-4564-9b13-e1acb82f1d0f",
   "metadata": {
    "id": "df5ab9e8-d555-4564-9b13-e1acb82f1d0f"
   },
   "source": [
    "## 3. ML Dataset construction\n",
    "Encoded text is divided into chunks to be fed into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a778319-d821-44c4-bd59-73ce26cb928b",
   "metadata": {
    "id": "3a778319-d821-44c4-bd59-73ce26cb928b"
   },
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "text_chunks = [text_encoded[i: i + chunk_size] for i in range(len(text_encoded) - chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ddadb-f5bf-4d27-8a4e-5fd0fe0d54cb",
   "metadata": {},
   "source": [
    "TextDataset class is created. It's instance, *seq_dataset*, stores the sequences samples and their corresponding targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc6376e8-a7cb-4613-815c-128c7192fdff",
   "metadata": {
    "id": "dc6376e8-a7cb-4613-815c-128c7192fdff"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d72533db-82e8-4212-91c0-a190339b9cca",
   "metadata": {
    "id": "d72533db-82e8-4212-91c0-a190339b9cca"
   },
   "outputs": [],
   "source": [
    "seq_dataset = TextDataset(torch.tensor(np.array(text_chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd585cfd-dcfe-4950-8272-66f7139bf83e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd585cfd-dcfe-4950-8272-66f7139bf83e",
    "outputId": "c2cc56ab-d215-4a26-c360-ce3e6fca23b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input (x):  'CONTENTS\\n\\nPreface\\n\\nPart I: The Special T'\n",
      "Model target (y): 'ONTENTS\\n\\nPreface\\n\\nPart I: The Special Th'\n",
      "Model input (x):  'ONTENTS\\n\\nPreface\\n\\nPart I: The Special Th'\n",
      "Model target (y): 'NTENTS\\n\\nPreface\\n\\nPart I: The Special The'\n",
      "Model input (x):  'NTENTS\\n\\nPreface\\n\\nPart I: The Special The'\n",
      "Model target (y): 'TENTS\\n\\nPreface\\n\\nPart I: The Special Theo'\n",
      "Model input (x):  'TENTS\\n\\nPreface\\n\\nPart I: The Special Theo'\n",
      "Model target (y): 'ENTS\\n\\nPreface\\n\\nPart I: The Special Theor'\n",
      "Model input (x):  'ENTS\\n\\nPreface\\n\\nPart I: The Special Theor'\n",
      "Model target (y): 'NTS\\n\\nPreface\\n\\nPart I: The Special Theory'\n",
      "Model input (x):  'NTS\\n\\nPreface\\n\\nPart I: The Special Theory'\n",
      "Model target (y): 'TS\\n\\nPreface\\n\\nPart I: The Special Theory '\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print('Model input (x): ', repr(''.join(char_array[seq])))\n",
    "    print('Model target (y):', repr(''.join(char_array[target])))\n",
    "    if i==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff935b-8261-4517-8f78-2cbba6b5341e",
   "metadata": {
    "id": "45ff935b-8261-4517-8f78-2cbba6b5341e"
   },
   "source": [
    "Next, a dataloader is created - which is an object used to pass data into the model in the form of \\'batches\\' (groups of specified size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66fee067-247f-4eae-bb65-711a3842bf68",
   "metadata": {
    "id": "66fee067-247f-4eae-bb65-711a3842bf68"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "batch_size = 64\n",
    "seq_dataloader = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590935f5-6a92-4ebc-8022-58ac3d1ff9ae",
   "metadata": {
    "id": "590935f5-6a92-4ebc-8022-58ac3d1ff9ae"
   },
   "source": [
    "## 4. Defining a RNN model\n",
    "\n",
    "*RNN* class defines RNN model's architecture:\n",
    "\n",
    "#### __init__ method:\n",
    "\n",
    "**self.embedding** is a table that stores an embedding vector for each unique character (we generate one embedding vector for each character by specifying vocab_size=len(char_array) below). Each embedding vector has length of embed_dim\n",
    "\n",
    "**self.rnn** is a neural network we will use. We specify its properties using *torch.nn.LSTM* function. LSTM stands for \\'Long Short-Term Memory\\' end indicates that we will use a RNN with LSTM cells used as hidden layers. The input to a hidden layer will be a specific character represented as an embedding vector. Therefore, we specify that we expect an input to be of size of the embedding vector length (embed_dim).\\\n",
    "The output of calling self.rnn is:\\\n",
    "*output_features,\\\n",
    "(final hidden state (for each element in the sequence),\\\n",
    "final cell state (for each element in the sequence))*\\\n",
    "It is utilized in *forward* method.\n",
    "\n",
    "**self.fc** is where we define a type of transformation we will apply to the output of hidden layers\n",
    "\n",
    "**self.rnn_hidden_size** is the number of features in the hidden state of RNN\n",
    "\n",
    "#### forward method:\n",
    "Define the computation performed at every model call (we can use this method because our RNN class inherits form class Module, which is a PyTorch Base class for all neural network modules)\n",
    "\n",
    "#### init_hidden method:\n",
    "Is where we initialize the state of a hidden layer and LSTM cell. (which will be used in forward method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3db9ef83-3367-4b4f-a16a-1aa8be171556",
   "metadata": {
    "id": "3db9ef83-3367-4b4f-a16a-1aa8be171556"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(input_size=embed_dim,\n",
    "                           hidden_size=rnn_hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
    "                            out_features=vocab_size)\n",
    "\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade0438-2cd6-4816-9912-c66958812ed0",
   "metadata": {
    "id": "dade0438-2cd6-4816-9912-c66958812ed0"
   },
   "source": [
    "## 5. Creating an instance of the defined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b111a04-4fe8-41fe-8b6f-1a826c908016",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b111a04-4fe8-41fe-8b6f-1a826c908016",
    "outputId": "c9227f83-d834-4418-dc23-e3bbe7a6970f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(87, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=87, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(char_array)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298a0ef-0e29-4507-b71e-32befe1d6888",
   "metadata": {
    "id": "9298a0ef-0e29-4507-b71e-32befe1d6888"
   },
   "source": [
    "define loss function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55a83abf-d50a-49d2-9a76-286c06b34c8d",
   "metadata": {
    "id": "55a83abf-d50a-49d2-9a76-286c06b34c8d"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374a6773-6145-4bc7-878b-81bf51972b49",
   "metadata": {
    "id": "374a6773-6145-4bc7-878b-81bf51972b49"
   },
   "source": [
    "## 6. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd47fb-7da7-4d61-acb7-9de96b70ee55",
   "metadata": {
    "id": "f2bd47fb-7da7-4d61-acb7-9de96b70ee55"
   },
   "source": [
    "Model is trained for multiple epochs. In each epoch only one batch is used.\\\n",
    "For each epoch we perform the following:\n",
    "1. hidden layer and cell states initialization\n",
    "2. use seq_dataloader as iterator object and load one batch (set of 64 input sequences and corresponding target sequences)\n",
    "3. reset the gradients of all optimized tensors.\n",
    "4. initialize loss (between predicted sequences and target sequences of loaded batch) as 0\n",
    "5. we use for loop to:\\\n",
    "   I. predict next character for each character in the input_sequence. It is done simultaneously for all input sequences in the batch.\\\n",
    "   II.Compute temporary loss as sum of losses for all characters\n",
    "6. Compute loss gradients after iterating through all characters.\n",
    "7. Perform optimization step to update model parameters (function .step() can be called once the gradients are computed using .backward())\n",
    "8. Compute final loss for a batch (dividing by the number of characters each sequence had)\n",
    "9. Printing current loss updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7942dfc-5a3f-4c32-a449-c683e59f249c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7942dfc-5a3f-4c32-a449-c683e59f249c",
    "outputId": "17745a5e-8aca-4cc2-fda9-d00242a23578"
   },
   "outputs": [],
   "source": [
    "if training_mode == 'perform_model_training':\n",
    "\n",
    "    num_epochs = 10000\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        hidden, cell = model.init_hidden(batch_size)\n",
    "        seq_batch, target_batch = next(iter(seq_dataloader))\n",
    "        seq_batch = seq_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        for c in range(seq_length):\n",
    "            pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
    "            loss += loss_fn(pred, target_batch[:, c])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()/seq_length\n",
    "        if epoch % 500 == 0:\n",
    "            print(f'Epoch {epoch} loss: {loss:.4f}')\n",
    "    print('time passed: ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9RJvCrlMd-f0",
   "metadata": {
    "id": "9RJvCrlMd-f0"
   },
   "source": [
    "## 7. Model saving and loading\n",
    "\n",
    "### I. saving and loading after training:\n",
    "\n",
    "There are 2 ways to save model training results:\\\n",
    "**method 1.** : saving the whole model object (i.e. model object of a specified architecture and its learned parameters)\\\n",
    "**method 2.** : saving just parameters (to reuse the parameters one needs to create a model object of the same architecture as the trained one and load them into the model)\n",
    "\n",
    "I'm using the **method 1.**, however, below I include commented code for **method 2.**.\n",
    "\n",
    "model.eval() function is called to indicate that the model will now be used in evaluation mode (i.e. for inference). It's because some layers behave differently during training and inference and need their mode to be set in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "RTG9MrqNleXJ",
   "metadata": {
    "id": "RTG9MrqNleXJ"
   },
   "outputs": [],
   "source": [
    "saving_method = 'method_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oqyrXOF4lnKr",
   "metadata": {
    "id": "oqyrXOF4lnKr"
   },
   "source": [
    "**method 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "-hXJv7uAcfQt",
   "metadata": {
    "id": "-hXJv7uAcfQt"
   },
   "outputs": [],
   "source": [
    "if training_mode == 'perform_model_training' and saving_method == 'method_1':\n",
    "    torch.save(model, 'data/models/self_trained_model.pt')\n",
    "\n",
    "    trained_model = torch.load('data/models/self_trained_model.pt', weights_only=False, map_location=device)\n",
    "    trained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vYzdBvJFlL90",
   "metadata": {
    "id": "vYzdBvJFlL90"
   },
   "source": [
    "**method 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "rKH3oxOQdJGv",
   "metadata": {
    "id": "rKH3oxOQdJGv"
   },
   "outputs": [],
   "source": [
    "# if training_mode == 'perform_model_training' and saving_method == 'method_2':\n",
    "#     torch.save(trained_model.state_dict(), 'data/models/self_trained_model_state_dict.pt')\n",
    "\n",
    "#     trained_model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
    "#     trained_model.load_state_dict(torch.load('data/models/self_trained_model_state_dict.pt', weights_only=False))\n",
    "#     trained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc7db75-e532-41d1-a4e6-9f126b0c3ce8",
   "metadata": {},
   "source": [
    "### II. loading a pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "QLkZkXF9lGBQ",
   "metadata": {
    "id": "QLkZkXF9lGBQ"
   },
   "outputs": [],
   "source": [
    "if training_mode == 'load_pretrained_model':\n",
    "    trained_model = torch.load('data/models/einstein_pretrained_model.pt', map_location=device, weights_only=False)\n",
    "    trained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WprMXBslykDt",
   "metadata": {
    "id": "WprMXBslykDt"
   },
   "source": [
    "## 8. Text generating function\n",
    "\n",
    "Below function is defined that utilizes the model to generate text on the basis of the book.\n",
    "\n",
    "The rate to with a generated text may be meaningful can be altered by changing a **predictability_factor** - the bigger the more predictable (and likely more meaningful) the generated text will be.\n",
    "\n",
    "Characters are added to starting string one at a time. Randomness is enabled by usage of Categorical() class and sample() function - the added character is not always the one with the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f47ff58d-4652-43ec-956b-58ea45da5297",
   "metadata": {
    "id": "f47ff58d-4652-43ec-956b-58ea45da5297"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, starting_str, len_generated_text=500, predictability_factor=2.0):\n",
    "    encoded_input = torch.tensor(\n",
    "        [char2int_encoding[s] for s in starting_str]\n",
    "    )\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1)).to(device)\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(\n",
    "            encoded_input[:, c].view(1), hidden, cell\n",
    "        )\n",
    "\n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_text):\n",
    "        logits, hidden, cell = model(\n",
    "            last_char.view(1), hidden, cell\n",
    "        )\n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        scaled_logits = logits * predictability_factor\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(char_array[last_char])\n",
    "\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PwfmRIlBGyHv",
   "metadata": {
    "id": "PwfmRIlBGyHv"
   },
   "source": [
    "## 9. Final use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "_IQu9w3OGwnw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_IQu9w3OGwnw",
    "outputId": "36fef066-703c-469c-851e-be14ab2addbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time and Space According to the statement of the\n",
      "time of a planet in its orbit should take place, and if the whole \" world-sphere.\"\n",
      "\n",
      "Perhaps the reader will\n",
      "doubtless admit that in reality such encounters constitute the only\n",
      "actual evidence to the\n",
      "clock is moving with the velocity v, which absorbs * an amount of energy E[0], then its inertial mass of a system of\n",
      "bodies can even be regarded as a Euclidean one, but that\n",
      "the latter theory has hitherto evinced.\n",
      "\n",
      "\n",
      "\n",
      "GENERALITY OF A \"FINITE\" AND YET \"UNBOUNDE\" UNIVE\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(trained_model, starting_str='Time and Space'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2b9624c-b4e5-4f21-9307-bc997e705747",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_IQu9w3OGwnw",
    "outputId": "36fef066-703c-469c-851e-be14ab2addbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time and Space According to the statement mayned on the laws of classical\n",
      "mechanics, we can satisfy this requirement for our illustration in the\n",
      "following form:\n",
      "\n",
      "                                                        x = ct\n",
      "\n",
      "or\n",
      "\n",
      "                                                                                                                                   x1 = wt1                                                                                                                                                 \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(trained_model, starting_str='Time and Space'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "389fb0b7-1c23-4f2e-9753-78b38957a94c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_IQu9w3OGwnw",
    "outputId": "36fef066-703c-469c-851e-be14ab2addbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time and Space according to the special principle of relativity has been\n",
      "justified, every intellect which strives after generalisation must\n",
      "feel the temptation that\n",
      "there exists for this surface by describing the latter theory has the law of the constancy of the velocity of the\n",
      "case in which the field equations of gravitation, if one was\n",
      "ready to drop hypothesis (1) without introducing the less natural\n",
      "cosmological term into the field equations of the Lorentz transformation is the cause of the accelerated ref\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(trained_model, starting_str='Time and Space'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbff3ef-d369-4728-9e34-a1006c82fd4c",
   "metadata": {},
   "source": [
    "## 10. Project benefits\n",
    "\n",
    "1. I used Google Colab for the first time\n",
    "2. I learned how to accelerate computations using Google-provisioned runtimes (and adapt the code to perform them)\n",
    "3. I used Git for project version control and GitHub Repository to share project files and reach them remotely\n",
    "4. First time I tried neural network training, results saving and loading\n",
    "5. I gained knowledge on data preporcessing, neural network construction, it's elements, working principle and underlying mechanisms \n",
    "6. I used PyTorch library\n",
    "7. Model succeeded in creating statements consisting of meaningful words (letters were added one at a time) on the basis of the book regarding physics. Physics-associated character of generated statements is highly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e454cf-9e36-4693-bc6b-3c71a54813bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
